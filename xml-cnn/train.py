import shutil
from argparse import ArgumentParser

import optuna
import torch
import yaml

from build_problem import BuildProblem
from my_functions import out_size
from timeit import default_timer as timer


# Convert params from Optuna
def convert_params(params, length):
    params["hidden_dims"] = 2 ** params["hidden_dims"]
    params["filter_channels"] = 2 ** params["filter_channels"]

    indexes = [i for i in range(params["num_filter_sizes"])]
    del params["num_filter_sizes"]

    params["filter_sizes"] = [params["filter_size_" + str(i)] for i in indexes]
    [params.pop("filter_size_" + str(i)) for i in indexes]

    filter_sizes = params["filter_sizes"]
    params["stride"] = [params["stride_" + str(i)] for i in indexes]
    [params.pop("stride_" + str(i)) for i in indexes]

    filter_channels = params["filter_channels"]
    args_list = zip(filter_sizes, params["stride"])
    out_sizes = [out_size(length, i, filter_channels, stride=j) for i, j in args_list]
    d_max_list = []
    for i, j in enumerate(out_sizes):
        n_list = [k for k in range(1, j + 1) if j % k < 1]
        n = params["d_max_pool_p_" + str(i)]
        d_max_list.append(n_list[n])

    [params.pop("d_max_pool_p_" + str(i)) for i in indexes]
    params["d_max_pool_p"] = d_max_list

    return params


def main():
    start = timer()
    # Config of Args
    parser = ArgumentParser()
    message = "Enable params search."
    action = "store_true"
    parser.add_argument("-s", "--params_search", help=message, action=action)
    message = "Use CPU."
    parser.add_argument("--use_cpu", help=message, action=action)
    args = parser.parse_args()
    is_ps = args.params_search
    is_ps = False
    use_cpu = args.use_cpu
    use_device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    use_device = use_cpu and torch.device("cpu") or use_device
    term_size = shutil.get_terminal_size().columns

    # Show learning mode
    out_str = is_ps and " Params Search Mode " or " Normal Train Mode "
    print(out_str.center(term_size, "="))

    # Load Params
    with open("params.yml") as f:
        params = yaml.safe_load(f)

    common = params["common"]
    hyper_params = params["hyper_params"]
    normal_train = params["normal_train"]
    params_search = params["params_search"]

    # Show Common Params
    print("\n" + " Params ".center(term_size, "-"))
    print([i for i in sorted(common.items())])
    print("-" * shutil.get_terminal_size().columns)

    # Set of automatically determined params
    params = {"device": use_device, "params_search": is_ps}

    common["model_cache_path"] += common["model_cache_path"][-1] == "/" and "" or "/"

    # is_ps = True
    # params["params_search"] = True
    if is_ps:
        params["test_data_path"] = None
        params.update(common)
        params.update(params_search)


    else:
        print("\n" + " Hyper Params ".center(term_size, "-"))
        print([i for i in sorted(hyper_params.items())])
        print("-" * shutil.get_terminal_size().columns)

        params.update(common)
        params.update(hyper_params)
        params.update(normal_train)

    # Build Problem
    trainer = BuildProblem(params)
    trainer.preprocess()

    # Run Training
    is_ps or trainer.run()

    end = timer()
    print("Total Runtime: " + (end - start).__str__())

    # For Optuna
    if is_ps:
        # Config of Optuna
        optuna.logging.disable_default_handler()
        study = optuna.create_study()

        # Params Search
        study.optimize(trainer.run, n_trials=params["trials"])

        trial = study.best_trial
        params = convert_params(trial.params, params["sequence_length"])

        print("\n\n" + " Best Hyper Params ".center(term_size, "-"))
        print([i for i in sorted(params.items())])
        print("-" * shutil.get_terminal_size().columns)


if __name__ == "__main__":
    main()
